{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Sparsity via Imputation into Filitered Numerical data from Positive Calss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = pd.read_table(\"Stage2_output_num.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# double check columns were removed.\n",
    "col_indexes_to_remove = [] \n",
    "for i in range(1,(pos.shape[1]-1)):\n",
    "    Count_the_Na = sum(pos.iloc[:,i].isnull())\n",
    "    #print Count_the_Na\n",
    "    if Count_the_Na == pos.shape[0]:\n",
    "        col_indexes_to_remove.append(i)\n",
    "print col_indexes_to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing best method of imputation for each column\n",
    "# Based on distribution of column \n",
    "\n",
    "If columns contain an appropriate number of Nas:\n",
    "- Columns with normal distribution will be imputed with mean\n",
    "- Columns with high skew are imputed with median - more strigent criteria applied to negative skew as most columns with negatively skewed\n",
    "- Else apply regression \n",
    "\n",
    "</br>\n",
    "Else imputation would be inappropriate if too many Nas in column - poor imputation could introduce bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "require_regression\n",
      "require_regression\n",
      "require_regression\n",
      "require_regression\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "require_regression\n",
      "require_regression\n",
      "require_regression\n",
      "require_regression\n",
      "require_regression\n",
      "require_regression\n",
      "impute median\n",
      "require_regression\n",
      "require_regression\n",
      "impute median\n",
      "require_regression\n",
      "impute median\n",
      "require_regression\n",
      "impute median\n",
      "require_regression\n",
      "impute median\n",
      "require_regression\n",
      "require_regression\n",
      "require_regression\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "require_regression\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "require_regression\n",
      "impute median\n",
      "require_regression\n",
      "require_regression\n",
      "require_regression\n",
      "require_regression\n",
      "impute median\n",
      "require_regression\n",
      "impute median\n",
      "require_regression\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "require_regression\n",
      "require_regression\n",
      "impute median\n",
      "require_regression\n",
      "require_regression\n",
      "require_regression\n",
      "require_regression\n",
      "impute median\n",
      "require_regression\n",
      "require_regression\n",
      "impute median\n",
      "require_regression\n",
      "require_regression\n",
      "impute median\n",
      "require_regression\n",
      "require_regression\n",
      "require_regression\n",
      "require_regression\n",
      "impute mean\n",
      "impute mean\n",
      "impute median\n",
      "impute median\n",
      "impute mean\n",
      "impute mean\n",
      "require_regression\n",
      "impute median\n",
      "require_regression\n",
      "impute median\n",
      "require_regression\n",
      "impute median\n",
      "impute mean\n",
      "impute mean\n",
      "require_regression\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "require_regression\n",
      "require_regression\n",
      "impute mean\n",
      "impute mean\n",
      "require_regression\n",
      "impute median\n",
      "impute mean\n",
      "impute mean\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "require_regression\n",
      "require_regression\n",
      "require_regression\n",
      "require_regression\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute mean\n",
      "require_regression\n",
      "impute median\n",
      "impute median\n",
      "require_regression\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute median\n",
      "impute mean\n",
      "impute mean\n",
      "impute median\n",
      "impute median\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "Too Sparse for imputation\n",
      "impute mean\n",
      "impute mean\n",
      "impute median\n",
      "impute median\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/scipy/stats/morestats.py:1327: UserWarning: Input data for shapiro has range zero. The results may not be accurate.\n",
      "  warnings.warn(\"Input data for shapiro has range zero. The results \"\n"
     ]
    }
   ],
   "source": [
    "indexes_to_impute_mean = []\n",
    "indexes_to_impute_median = []\n",
    "indexes_for_regression = []\n",
    "indexes_for_toManyNas = []\n",
    "\n",
    "for i in range(1,(pos.shape[1]-1)):  # Exclude id and response variables from loop\n",
    "    column = pos.iloc[:,i].dropna()\n",
    "    \n",
    "    shapiro_p_val = stats.shapiro(column)[1]  # if p value is more than 0.05 = could be normal \n",
    "    range_descript = column.describe()\n",
    "    IQ_ratio_R = (range_descript[6] - range_descript[4]) / (range_descript[7] - range_descript[3])\n",
    "    \n",
    "    # Ratio of Nas to non Nas\n",
    "    Na_ratio = pos.iloc[:,i].isnull().sum() / np.uint64(pos.iloc[:,i].shape[0])\n",
    "    if (Na_ratio < 0.3):\n",
    "        if (shapiro_p_val > 0.05):\n",
    "            # if less than 0.05, we cannot reject H0 that sample comes from a normal distribution\n",
    "            # so if shapiro is more than 0.05, impute mean.\n",
    "            indexes_to_impute_mean.append(i)\n",
    "            print \"impute mean\"\n",
    "        elif (skew(column) > 1.5): # if positive skew inpute median\n",
    "            indexes_to_impute_median.append(i)\n",
    "            print \"impute median\"\n",
    "        elif (skew(column) < 1.75) and (kurtosis(column) > 2): # if negative skew with high kurtosis, impute median\n",
    "            indexes_to_impute_median.append(i)\n",
    "            print \"impute median\"\n",
    "        elif (skew(column) < 2) and (IQ_ratio_R < 0.2):\n",
    "            indexes_to_impute_median.append(i)\n",
    "            print \"impute median\"\n",
    "        else:\n",
    "            print \"require_regression\"\n",
    "            indexes_for_regression.append(i)\n",
    "    else: \n",
    "        indexes_for_toManyNas.append(i)\n",
    "        print \"Too Sparse for imputation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes for Mean Imputation: [91, 92, 95, 96, 103, 104, 111, 112, 115, 116, 140, 149, 150, 161, 162] \n",
      "\n",
      "Indexes for Median Imputation: [22, 23, 24, 25, 32, 35, 37, 39, 41, 45, 46, 47, 49, 50, 51, 52, 53, 55, 60, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 80, 83, 86, 93, 94, 98, 100, 102, 106, 107, 108, 114, 117, 118, 119, 120, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 142, 143, 145, 146, 147, 148, 151, 152, 163, 164] \n",
      "\n",
      "Indexes for Regression Imputation: [18, 19, 20, 21, 26, 27, 28, 29, 30, 31, 33, 34, 36, 38, 40, 42, 43, 44, 48, 54, 56, 57, 58, 59, 61, 63, 73, 74, 76, 77, 78, 79, 81, 82, 84, 85, 87, 88, 89, 90, 97, 99, 101, 105, 109, 110, 113, 121, 122, 123, 124, 141, 144] \n",
      "\n",
      "Leave Alone: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 153, 154, 155, 156, 157, 158, 159, 160] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Indexes for Mean Imputation: %s \\n\" % (indexes_to_impute_mean))\n",
    "print(\"Indexes for Median Imputation: %s \\n\" % (indexes_to_impute_median))\n",
    "print(\"Indexes for Regression Imputation: %s \\n\" % (indexes_for_regression))\n",
    "print(\"Leave Alone: %s \\n\" % (indexes_for_toManyNas))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation of columns with with at most 0.3 ratio between Na vs NonNa\n",
    "Imputation may help deal with sparsitiy - but it can also introduce bias <br>\n",
    "Thus we will sample only 75 % of these initial chosen features to impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexes_to_impute_mean = random.sample(indexes_to_impute_mean, int(round(len(indexes_to_impute_mean)*0.75))) # no replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexes_to_impute_median = random.sample(indexes_to_impute_median, int(round(len(indexes_to_impute_median)*0.75))) # no replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexes_for_regression = random.sample(indexes_for_regression, int(round(len(indexes_for_regression)*0.75))) # no replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute median values for skewed columns with high kurtosis / low ratio between interquartile range and range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in indexes_to_impute_median:\n",
    "    fillIt = pos.iloc[:,i].dropna().median()\n",
    "    pos.iloc[:,i] = pos.iloc[:,i].fillna(value=fillIt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute mean values for columns that was unable to reject the null hypothesis of Shapiro-wilks test for normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in indexes_to_impute_mean:\n",
    "    fillIt = pos.iloc[:,i].dropna().mean()\n",
    "    pos.iloc[:,i] = pos.iloc[:,i].fillna(value=fillIt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation via regression techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models used:\n",
    "## 1- Linear Regression\n",
    "## 2- Bayesian Ridge\n",
    "## 3- AdaBoost Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg1 = linear_model.LinearRegression()\n",
    "reg2 = linear_model.BayesianRidge()\n",
    "reg3 = AdaBoostRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setting up feature selection and imputer\n",
    "selector = RFE(reg1, 10, step=1)\n",
    "imputer = Imputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Thus function applies a regression model\n",
    "# Then it calculates and returns SSR from resubtitution to Non-Na values to y (target) \n",
    "# Then it returns the predictions for the all values of Y\n",
    "\n",
    "def RegressionModel(XIn,Xtest, YIn, clfIn, doPCABool=False):\n",
    "    YYflattened = YIn.values.flatten()\n",
    " \n",
    "    if (doPCABool == True): \n",
    "        x = StandardScaler().fit_transform(XIn)\n",
    "        pca = PCA(n_components=int(round(XIn.shape[1]/2)))\n",
    "        principalComponents = pca.fit_transform(x) # transform Train set using PCA\n",
    "        \n",
    "        clfIn.fit(principalComponents, YYflattened) # predict Y using trainset\n",
    "        \n",
    "        Resubstitution_preds = clfIn.predict(principalComponents)  # predict Y using trainset PCA'd\n",
    "        \n",
    "        x_pca = StandardScaler().fit_transform(Xtest) \n",
    "        imputed_pcad = pca.fit_transform(x_pca) \n",
    "        Generalized_preds = clfIn.predict(imputed_pcad)\n",
    "        \n",
    "    else:\n",
    "        clfIn.fit(XIn, YYflattened) # train X train set to predict Y\n",
    "        Resubstitution_preds = clfIn.predict(XIn) # Predict y using train set\n",
    "        Generalized_preds = clfIn.predict(Xtest) # predict y using IMPUTED TEST set\n",
    "\n",
    "\n",
    "    resids =(YYflattened - Resubstitution_preds)**2  # Test resubstution error\n",
    "    \n",
    "    return [sum(resids), Generalized_preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict each suitable column with model that returned best resubstitution error \n",
    "### For each model, attempt model again with PCA input / features selected from recursive selection with linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allIndexes = set(range(1,(pos.shape[1]-1))) # all indexes\n",
    "\n",
    "# all indexes take away union of too many nan and reg indexes\n",
    "AverageImputedCols = allIndexes - set(indexes_for_regression).union(indexes_for_toManyNas) \n",
    "\n",
    "for i in indexes_for_regression:\n",
    "    Yy = pos.iloc[:,i] # test set\n",
    "    predictor_Col = {i} # column to predict\n",
    "    trainIndexes =  AverageImputedCols - predictor_Col # train indexes - indexes other than the predictor column\n",
    "    trainIndexes = list(trainIndexes) \n",
    "    Xx = pos.iloc[:,trainIndexes] # train set with NAS\n",
    "    xxyy = pd.concat([Xx, Yy], axis=1) # all set\n",
    "    xxyy_no_na = xxyy.dropna(axis=\"index\") # drop nas by index.\n",
    "    YY = xxyy_no_na[[Yy.name]] # y set with NO Nas\n",
    "    XX = xxyy_no_na.loc[:, xxyy_no_na.columns != Yy.name] # train set with NO nas\n",
    "    \n",
    "    Xx_imputed = imputer.fit_transform(pd.DataFrame(Xx)) # uses mean/median/mode\n",
    "    Xx_imputed = pd.DataFrame(Xx_imputed, columns=[Xx.columns])\n",
    "    \n",
    "    ### Apply models \n",
    "    m1 = RegressionModel(XIn=XX,Xtest=Xx_imputed, YIn=YY, clfIn=reg1) # Linear Regression\n",
    "    m2 = RegressionModel(XIn=XX,Xtest=Xx_imputed, YIn=YY, clfIn=reg2) # Bayesian Ridge\n",
    "    m3 = RegressionModel(XIn=XX,Xtest=Xx_imputed, YIn=YY, clfIn=reg3) # AdaBoostRegressor\n",
    "    m4 = RegressionModel(XIn=XX,Xtest=Xx_imputed, YIn=YY, clfIn=reg1, doPCABool=True) \n",
    "    m5 = RegressionModel(XIn=XX,Xtest=Xx_imputed, YIn=YY, clfIn=reg2, doPCABool=True)\n",
    "    m6 = RegressionModel(XIn=XX,Xtest=Xx_imputed, YIn=YY, clfIn=reg3, doPCABool=True)\n",
    "    # Feature selection may take too much time\n",
    "    # so will do one FS - recursive selection with linear regression.\n",
    "    selector = selector.fit(XX, YY.values.flatten())\n",
    "    FS_XX = XX[XX.columns[selector.support_]]\n",
    "    FS_Xx_Imputed = Xx_imputed[Xx_imputed.columns[selector.support_]]\n",
    "    \n",
    "    m7 = RegressionModel(XIn=FS_XX,Xtest=FS_Xx_Imputed, YIn=YY, clfIn=reg1) # Linear Regression with FS\n",
    "    m8 = RegressionModel(XIn=FS_XX,Xtest=FS_Xx_Imputed, YIn=YY, clfIn=reg2) # Bayesian Ridge with FS\n",
    "    m9 = RegressionModel(XIn=FS_XX,Xtest=FS_Xx_Imputed, YIn=YY, clfIn=reg3) # AdaBoostRegressor with FS\n",
    "    m10 = RegressionModel(XIn=FS_XX,Xtest=FS_Xx_Imputed, YIn=YY, clfIn=reg1, doPCABool=True)# PC's cut with FS\n",
    "    m11 = RegressionModel(XIn=FS_XX,Xtest=FS_Xx_Imputed, YIn=YY, clfIn=reg2, doPCABool=True) # rinse, repeat\n",
    "    m12 = RegressionModel(XIn=FS_XX,Xtest=FS_Xx_Imputed, YIn=YY, clfIn=reg3, doPCABool=True)\n",
    "    \n",
    "    ssr_collect = [m1[0], m2[0], m3[0], m4[0], m5[0], m6[0],\n",
    "                        m7[0], m8[0], m9[0], m10[0], m11[0], m12[0]]\n",
    "    pred_collect = [m1[1], m2[1], m3[1], m4[1], m5[1], m6[1],\n",
    "                    m7[1], m8[1], m9[1], m10[1], m11[1], m12[1]]\n",
    "    \n",
    "    # find model which produced the smallest sum of square residuals\n",
    "    sorted_ssr, sorted_pred = zip(*sorted(zip(ssr_collect, pred_collect)))\n",
    "    best_pred = sorted_pred[0] # best ssr\n",
    "    \n",
    "    output =pd.DataFrame(best_pred, columns=[\"s\"])\n",
    "    for j in range(0,pos.shape[0]):\n",
    "        pos.iloc[j,i] = output.iloc[j,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out partially imputed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos.to_csv(\"Stage3_num_partially_IMPUTED.csv\", sep='\\t', index= False )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
