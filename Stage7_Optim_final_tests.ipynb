{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "# Ensembl methods: Adaboost & Gradient Boosting & RF\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.cross_validation import train_test_split \n",
    "\n",
    "#Artificial Neural Network packages\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in high quality imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Negatives = pd.read_csv('IMPUTED_NEGs.csv', sep=\"\\t\") #index_col=False\n",
    "Positive_cat_dat = pd.read_csv('IMP_pos_datcat.csv', sep=\"\\t\") #index_col=False\n",
    "Positive_num = pd.read_csv('IMPUTED_pos_num.csv', sep=\"\\t\") #index_col=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Crème de la crème"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A little preprocessing.\n",
    "new_df = pd.merge(Positive_cat_dat, Positive_num, on=Positive_num.Id)\n",
    "new_df = new_df.drop(['Id_y', 'Response_y'], axis=1)\n",
    "Positives = new_df.rename(columns={'Id_x': 'Id', 'Response_x': 'Response'})\n",
    "#set(Negatives.columns) - set(new_df.columns)\n",
    "Negatives = Negatives.drop(['Id.1', 'Response.1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = random.sample(Negatives.index, 20000)\n",
    "Negative_s = Negatives.ix[rows]\n",
    "Negative_HODL = Negatives.drop(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataframe of 1815 high quality positives and 20,000 samples of high quality negatives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine positive and negatives\n",
    "frames = [Negative_s, Positives]\n",
    "data = pd.concat(frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data from original sparse data set - to preturb data quality and guard the generalization ability of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_negatives = pd.read_csv('All_50000samps_negatives_all_quality.csv', sep=\"\\t\") #index_col=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates between high quality and low quality negative dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_negatives = bad_negatives.drop('Unnamed: 0', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common = bad_negatives['Id'].isin(Negative_s['Id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_neg = bad_negatives[common==False] #.shape # if true, remove from bad_negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find categorical columns with multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cat_Head = pd.read_csv('CAT_HEADS.csv', sep=\",\") #index_col=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>L0_S1_F25</th>\n",
       "      <th>L0_S1_F27</th>\n",
       "      <th>L0_S1_F29</th>\n",
       "      <th>L0_S1_F31</th>\n",
       "      <th>L0_S2_F33</th>\n",
       "      <th>L0_S2_F35</th>\n",
       "      <th>L0_S2_F37</th>\n",
       "      <th>L0_S2_F39</th>\n",
       "      <th>L0_S2_F41</th>\n",
       "      <th>...</th>\n",
       "      <th>L3_S49_F4225</th>\n",
       "      <th>L3_S49_F4227</th>\n",
       "      <th>L3_S49_F4229</th>\n",
       "      <th>L3_S49_F4230</th>\n",
       "      <th>L3_S49_F4232</th>\n",
       "      <th>L3_S49_F4234</th>\n",
       "      <th>L3_S49_F4235</th>\n",
       "      <th>L3_S49_F4237</th>\n",
       "      <th>L3_S49_F4239</th>\n",
       "      <th>L3_S49_F4240</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 2141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Id, L0_S1_F25, L0_S1_F27, L0_S1_F29, L0_S1_F31, L0_S2_F33, L0_S2_F35, L0_S2_F37, L0_S2_F39, L0_S2_F41, L0_S2_F43, L0_S2_F45, L0_S2_F47, L0_S2_F49, L0_S2_F51, L0_S2_F53, L0_S2_F55, L0_S2_F57, L0_S2_F59, L0_S2_F61, L0_S2_F63, L0_S2_F65, L0_S2_F67, L0_S3_F69, L0_S3_F71, L0_S3_F73, L0_S3_F75, L0_S3_F77, L0_S3_F79, L0_S3_F81, L0_S3_F83, L0_S3_F85, L0_S3_F87, L0_S3_F89, L0_S3_F91, L0_S3_F93, L0_S3_F95, L0_S3_F97, L0_S3_F99, L0_S3_F101, L0_S3_F103, L0_S4_F105, L0_S4_F107, L0_S4_F108, L0_S4_F110, L0_S4_F112, L0_S4_F113, L0_S6_F119, L0_S6_F121, L0_S6_F123, L0_S6_F125, L0_S6_F126, L0_S6_F128, L0_S6_F129, L0_S6_F131, L0_S6_F133, L0_S6_F135, L0_S9_F151, L0_S9_F153, L0_S9_F154, L0_S9_F156, L0_S9_F158, L0_S9_F159, L0_S9_F161, L0_S9_F163, L0_S9_F164, L0_S9_F166, L0_S9_F168, L0_S9_F169, L0_S9_F171, L0_S9_F173, L0_S9_F174, L0_S9_F176, L0_S9_F178, L0_S9_F179, L0_S9_F181, L0_S9_F183, L0_S9_F184, L0_S9_F186, L0_S9_F188, L0_S9_F189, L0_S9_F191, L0_S9_F193, L0_S9_F194, L0_S9_F196, L0_S9_F198, L0_S9_F199, L0_S9_F201, L0_S9_F203, L0_S9_F204, L0_S9_F206, L0_S9_F208, L0_S9_F209, L0_S9_F211, L0_S9_F213, L0_S9_F214, L0_S10_F215, L0_S10_F217, L0_S10_F218, L0_S10_F220, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 2141 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cat_Head.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_cat_head = bad_neg[Cat_Head.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Here we show that all other columns other than Id has only binary categories...\n",
    "# This means there is no point in trying to fill in Nas because only one class exist. \n",
    "duplicateCount = bad_cat_head.T.apply(lambda x: x.nunique(), axis=1) # Here we show that \n",
    "multiple_cat_heads = duplicateCount[duplicateCount > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multiple_cat_heads = multiple_cat_heads.drop(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multiple_bad_cat_heads = bad_neg[multiple_cat_heads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_neg_picked = bad_neg[Positives.columns] # chosen by positive feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Neg_result = pd.concat([bad_neg_picked, multiple_bad_cat_heads], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from low quality negative samples with informative categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = random.sample(Neg_result.index, 10000)\n",
    "Negative_cat = Neg_result.ix[rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>L3_S29_F3317</th>\n",
       "      <th>L3_S29_F3320</th>\n",
       "      <th>L3_S29_F3323</th>\n",
       "      <th>L3_S29_F3326</th>\n",
       "      <th>L3_S29_F3329</th>\n",
       "      <th>L3_S29_F3332</th>\n",
       "      <th>L3_S29_F3335</th>\n",
       "      <th>L3_S29_F3338</th>\n",
       "      <th>L3_S29_F3341</th>\n",
       "      <th>...</th>\n",
       "      <th>L0_S2_F37</th>\n",
       "      <th>L0_S2_F61</th>\n",
       "      <th>L0_S1_F31</th>\n",
       "      <th>L0_S1_F31</th>\n",
       "      <th>L0_S1_F31</th>\n",
       "      <th>L0_S1_F31</th>\n",
       "      <th>L0_S1_F29</th>\n",
       "      <th>L0_S1_F29</th>\n",
       "      <th>L0_S1_F29</th>\n",
       "      <th>L0_S1_F29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3644</th>\n",
       "      <td>51272</td>\n",
       "      <td>T1</td>\n",
       "      <td>T1</td>\n",
       "      <td>T1</td>\n",
       "      <td>T1</td>\n",
       "      <td>T1</td>\n",
       "      <td>T1</td>\n",
       "      <td>T1</td>\n",
       "      <td>T1</td>\n",
       "      <td>T1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 438 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id L3_S29_F3317 L3_S29_F3320 L3_S29_F3323 L3_S29_F3326 L3_S29_F3329  \\\n",
       "3644  51272           T1           T1           T1           T1           T1   \n",
       "\n",
       "     L3_S29_F3332 L3_S29_F3335 L3_S29_F3338 L3_S29_F3341    ...    L0_S2_F37  \\\n",
       "3644           T1           T1           T1           T1    ...          NaN   \n",
       "\n",
       "     L0_S2_F61 L0_S1_F31 L0_S1_F31 L0_S1_F31 L0_S1_F31 L0_S1_F29 L0_S1_F29  \\\n",
       "3644       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "     L0_S1_F29 L0_S1_F29  \n",
       "3644       NaN       NaN  \n",
       "\n",
       "[1 rows x 438 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Negative_cat.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 438)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Negative_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perturb high quality negative samples with low quality samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 346)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Negative_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_negative_cols = list(set(Negative_s.columns).intersection(set(Negative_cat.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.merge(Negative_cat, Negative_s, how='outer', on=common_negative_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat for Positive samples - add in loq quality data + categorical columns containing multiple entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1815, 346)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Positives.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in low quality Positive classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_positives = pd.read_csv('All_positives_all_quality.csv', sep=\"\\t\") #index_col=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common = bad_positives['Id'].isin(Positives['Id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_pos = bad_positives[common==False] # Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_positives = bad_pos[Positives.columns] ## .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Perturbed_Positives = pd.concat([bad_positives, Positives])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns_to_drop = set(Perturbed_Negatives.columns) - set(Perturbed_Positives.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Perturbed_Neg = Perturbed_Negatives.drop(columns_to_drop, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Perturbed_Neg.to_csv(\"Perturbed_Negatives.csv\", sep='\\t', index= False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Perturbed_Positives.to_csv(\"Perturbed_Positives.csv\", sep='\\t', index= False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data in train-test set and HODL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Perturbed = pd.concat([Perturbed_Neg, Perturbed_Positives])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Perturbed.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Perturbed = Perturbed.replace(['T1'], [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Perturbed = Perturbed.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Perturbed_X = Perturbed.drop(['Response'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Perturbed_Y = Perturbed[['Response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_trainTest, X_HODL, y_trainTest, y_HODL = train_test_split(Perturbed_X, Perturbed_Y , test_size= 0.2, random_state = 888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_trainTest = X_trainTest.join(y_trainTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ALL_HODL = X_HODL.join(y_HODL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28729, 346)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_trainTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6594, 346)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_HODL.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DownSample_Negatives(AllData, Num_samps):\n",
    "    neg_indices = AllData[AllData.Response == 0].index # all indexes of negative samples\n",
    "    samp_neg_indices = np.random.choice(neg_indices, Num_samps, replace=False) # sample from neg indices\n",
    "    down_samp_negs = AllData.loc[samp_neg_indices] # retrieve from data\n",
    "    return(down_samp_negs)\n",
    "def DownSample_Positives(AllData, Num_samps):\n",
    "    pos_indices = AllData[AllData.Response == 1].index # all indexes of negative samples\n",
    "    samp_pos_indices = np.random.choice(pos_indices, Num_samps, replace=False) # sample from neg indices\n",
    "    down_samp_pos = AllData.loc[samp_pos_indices] # retrieve from data\n",
    "    return(down_samp_pos)\n",
    "def DownSample(AllData, Num_samps):\n",
    "    neg_indices = AllData.index # all indexes of negative samples\n",
    "    samp_neg_indices = np.random.choice(neg_indices, Num_samps, replace=False) # sample from neg indices\n",
    "    down_samp_negs = AllData.loc[samp_neg_indices] # retrieve from data\n",
    "    return(down_samp_negs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_TT = ALL_trainTest.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainTest_neg_downSamp = DownSample_Negatives(ALL_TT,3500)\n",
    "#Trainy[Trainy.Response == 1].shape\n",
    "#Testy = DownSample(data,2000)\n",
    "#Testy[Testy.Response == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 347)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainTest_neg_downSamp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainTest_pos = ALL_TT[ALL_TT.Response == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3413, 347)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainTest_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_TT = pd.concat([trainTest_neg_downSamp, trainTest_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Chosen Models\n",
    "\n",
    "## - Random Forrest\n",
    "## - Gradient Boosting\n",
    "## - Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DoRForest(X_trainIn, y_trainIn, X_testIn, DepthIn, MaxFeatIn):\n",
    "    #model = RandomForestClassifier(criterion=CriterionIn, n_estimators = DepthIn, max_features= MaxFeatIn)\n",
    "    model = RandomForestClassifier(criterion=\"entropy\", n_estimators = DepthIn, max_features= MaxFeatIn)\n",
    "    model.fit(X_trainIn, y_trainIn)\n",
    "    RF_y_predictions = model.predict(X_testIn)\n",
    "    return(RF_y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to create a confusion matrix\n",
    "def DoConfMat(yTestIn, yPredIn):\n",
    "    Confusion_M1 = confusion_matrix(yTestIn, yPredIn) #.ravel()\n",
    "    tn = Confusion_M1[0][0]\n",
    "    fp = Confusion_M1[0][1]\n",
    "    fn = Confusion_M1[1][0]\n",
    "    tp = Confusion_M1[1][1]\n",
    "    \n",
    "    MCC= (float((tp*tn))-(fp*fn)) / np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    ALL1 = tn + fp + fn + tp \n",
    "    stat_accuracy1 = float(tp + tn)/ALL1\n",
    "    stat_misclassification_rate1 = float(fp + fn) / ALL1\n",
    "    stat_false_negative1 = float(fn) / (tp + fn) # false negative rate\n",
    "    stat_false_positive1 = float(fp) / (fp + tn) # false positive rate\n",
    "    stat_sensitivity1 = float(tp) / (tp + fn) # false negative rate\n",
    "    stat_specificity1 = float(tn) / (tn + fp)\n",
    "    stat_precision1  = float(tp)/(tp+fp)\n",
    "    \n",
    "    return(stat_accuracy1, stat_misclassification_rate1, stat_sensitivity1, stat_false_positive1, stat_specificity1, stat_false_negative1, stat_precision1, MCC)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def TrainClassifier(costIn, gammaIn, randomSplit):\n",
    "\n",
    "def cross_validate_roc(mydataIn, num_k_param, param1, param2):\n",
    "    mydataIn = mydataIn.sample(frac=1) # randomly shuffle data, frac parameter define what fraction of data to return - frac= 1  means we return all data shuffled\n",
    "    folds = np.array_split(mydataIn, num_k_param)\n",
    "    num_k = int(len(folds)) \n",
    "    CV_accuracy = []\n",
    "    \n",
    "    # K fold cross validation\n",
    "    for i in range(0,num_k):\n",
    "        Ktest = folds[i]\n",
    "        Ktrain = []\n",
    "        for j in xrange(len(folds)):\n",
    "            if j != i:\n",
    "                Ktrain.append(folds[j])  \n",
    "        Ktrain = pd.concat(Ktrain)\n",
    "        \n",
    "        KX_train = Ktrain.drop('Response', 1)         \n",
    "        KX_test = Ktest.drop('Response',1)              \n",
    "        Ky_train = Ktrain['Response'] \n",
    "        Ky_test = Ktest['Response'] \n",
    "    \n",
    "        #outty = DoGB(X_trainIn=KX_train, y_trainIn=Ky_train, X_testIn=KX_test)\n",
    "        #outty = DoSVM(X_trainIn=KX_train, y_trainIn=Ky_train, X_testIn=KX_test, costIn=costIn, gammaIn=gammaIn)\n",
    "        outty= DoRForest(X_trainIn=KX_train, y_trainIn=Ky_train, X_testIn=KX_test, DepthIn=param1, MaxFeatIn=param2)\n",
    "        \n",
    "        Ky_predictions = outty\n",
    "        \n",
    "        # ACCURACY\n",
    "        #Kclassification_stats = accuracy_score(y_true=Ky_test, y_pred=Ky_predictions)\n",
    "        #CV_accuracy.append(Kclassification_stats)\n",
    "        \n",
    "        # MCC\n",
    "        # \n",
    "        Kclassification_stats = DoConfMat(yTestIn=Ky_test, yPredIn=Ky_predictions)\n",
    "        CV_accuracy.append(Kclassification_stats[7])\n",
    "\n",
    "    stat_accuracy2 = sum(CV_accuracy)/len(CV_accuracy)\n",
    "    return stat_accuracy2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize a Random Forrest with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD9CAYAAACC7q1lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA1iSCjGAJakQA1iSCvk/poIzAnfmqWwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125eb4350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MaxFeat = [10, 30, 60, 90]\n",
    "MaxDepth = [10, 30, 60, 90]\n",
    "\n",
    "def GridSearch(P_List1, P_List2):\n",
    "    acc_row = []\n",
    "    for i in (P_List1):\n",
    "        acc_columns = []\n",
    "        for j in (P_List2):\n",
    "            acc =cross_validate_roc(mydataIn=ALL_TT, num_k_param=3, param1=i, param2=j)\n",
    "            acc_columns.append(acc)\n",
    "        acc_row.append(acc_columns)\n",
    "        \n",
    "    heat = sns.heatmap(acc_row)\n",
    "    return acc_row\n",
    "\n",
    "GridOut = GridSearch(P_List1=MaxFeat, P_List2=MaxDepth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80541725363113958"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate_roc(mydataIn=ALL_TT, num_k_param=3, param1=30, param2=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Random Forest Parameters\n",
    "### DepthIn=30\n",
    "### MaxFeatIn=90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6913, 345)"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_TT.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6594, 345)"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_HODL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_T_Target = all_TT[['Response']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_T_Feats = all_TT.drop(['Response'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_HODL_Target = all_HODL[['Response']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_HODL_Feats = all_HODL.drop(['Response'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF_out = DoRForest(X_trainIn=all_T_Feats, y_trainIn=all_T_Target.values.ravel(), X_testIn=all_HODL_Feats, DepthIn=30, MaxFeatIn=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7388535031847133,\n",
       " 0.2611464968152866,\n",
       " 0.9810606060606061,\n",
       " 0.28222881635344543,\n",
       " 0.7177711836465546,\n",
       " 0.01893939393939394,\n",
       " 0.23228699551569507,\n",
       " 0.40090919262370722)"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DoConfMat(yTestIn=all_HODL_Target.values.ravel(), yPredIn=BSUS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Gradient Boosting with Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DoGB(X_trainIn, y_trainIn, X_testIn, a,b,c,d):\n",
    "    model = GradientBoostingClassifier(min_samples_split= a, max_depth=b, min_weight_fraction_leaf=c, max_features=d)\n",
    "    model.fit(X_trainIn, y_trainIn)\n",
    "    D_y_predictions = model.predict(X_testIn)\n",
    "    return(D_y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_roc(mydataIn, num_k_param, a,b,c,d):\n",
    "    mydataIn = mydataIn.sample(frac=1) # randomly shuffle data, frac parameter define what fraction of data to return - frac= 1  means we return all data shuffled\n",
    "    folds = np.array_split(mydataIn, num_k_param)\n",
    "    num_k = int(len(folds)) \n",
    "    CV_accuracy = []\n",
    "    \n",
    "    # K fold cross validation\n",
    "    for i in range(0,num_k):\n",
    "        Ktest = folds[i]\n",
    "        Ktrain = []\n",
    "        for j in xrange(len(folds)):\n",
    "            if j != i:\n",
    "                Ktrain.append(folds[j])  \n",
    "        Ktrain = pd.concat(Ktrain)\n",
    "        \n",
    "        KX_train = Ktrain.drop('Response', 1)         \n",
    "        KX_test = Ktest.drop('Response',1)              \n",
    "        Ky_train = Ktrain['Response'] \n",
    "        Ky_test = Ktest['Response'] \n",
    "    \n",
    "        outty = DoGB(X_trainIn=KX_train, y_trainIn=Ky_train, X_testIn=KX_test,a=a, b=b, c=c, d=d)\n",
    "        Ky_predictions = outty\n",
    "\n",
    "        # MCC\n",
    "        # \n",
    "        Kclassification_stats = DoConfMat(yTestIn=Ky_test, yPredIn=Ky_predictions)\n",
    "        CV_accuracy.append(Kclassification_stats[7])\n",
    "\n",
    "    stat_accuracy2 = sum(CV_accuracy)/len(CV_accuracy)\n",
    "    return stat_accuracy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# min_samples_split (a) = 1- 4000 # number of samples to use per split\n",
    "# max depth of a tree (b) = 5-100\n",
    "# min_weight_fraction_leaf (c) = 0.1-0.5\n",
    "# max feat (d) =  should be 30% - 40% of 347\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RandomSearch():\n",
    "    acc_collect = []\n",
    "    params = []\n",
    "    for i in range(0,10):\n",
    "        a1 = int(round(np.random.uniform(low=40, high=2000, size=1)))\n",
    "        b1 = int(round(np.random.uniform(low=5, high=100, size=1)))\n",
    "        c1 = float(np.random.uniform(low=0.1, high=0.5, size=1))\n",
    "        d1 = int(round(np.random.uniform(low=100, high=200, size=1)))\n",
    "        acc = cross_validate_roc(mydataIn=ALL_TT, num_k_param=3, a=a1,b=b1,c=c1,d=d1)\n",
    "        params.append([a1,b1,c1,d1])\n",
    "        acc_collect.append(acc)\n",
    "    sorted_acc, params_out = zip(*sorted(zip(acc_collect, params)))\n",
    "    #print sorted_acc\n",
    "    fin_a = params_out[-1][0]\n",
    "    fin_b = params_out[-1][1]\n",
    "    fin_c = params_out[-1][2]\n",
    "    fin_d = params_out[-1][3]\n",
    "    return [fin_a, fin_b, fin_c, fin_d, sorted_acc[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[964, 90, 0.2750062705238385, 132, 0.87881727104103902]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[948, 34, 0.16580295996847916, 120, 0.87967123153224647]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomSearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Grandient Boost params:\n",
    "### min_samples_split (a) = 948\n",
    "### max depth of a tree (b) = 34\n",
    "### min_weight_fraction_leaf (c) = 0.165\n",
    "### max feat = 0.878"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ANN(X_train, y_train, X_test):\n",
    "    X_shape = X_train.shape  # IMPORTANT FOR INPUT LAYER PARAMETER!!!!\n",
    "    num_features = X_shape[1]\n",
    "    #initialize ANN by defining it as a sequence of layers\n",
    "    model = Sequential()\n",
    "    #Adding input layer\n",
    "    #activation function parameter set to 'relu' = rectifier function \n",
    "    #uniform = intialization method\n",
    "    model.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = num_features))\n",
    "    # Add 3 hidden layer\n",
    "    model.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "    model.add(Dense(output_dim = 6, init = 'uniform', activation = 'tanh'))\n",
    "    model.add(Dense(output_dim = 6, init = 'uniform', activation = 'linear'))\n",
    "\n",
    "    # Add output layer\n",
    "    model.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "    # Apply stochastic gradient descent\n",
    "    model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "    # Choose number of epochs \n",
    "\n",
    "    model.fit(X_train.values, y_train.values, batch_size=30, nb_epoch = 15)\n",
    "    y_predictions = model.predict(X_test.values)\n",
    "    y_predictions = (y_predictions > 0.5)  #if y_pred is larger than 0.5, it returns CANCER TRUE, if not, returns false\n",
    "    return y_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_T_Target = ALL_TT[['Response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_T_Feats = ALL_TT.drop(['Response'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_T_Feats = ALL_T_Feats.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_T_Feats = ALL_T_Feats.drop(['Id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HODL_Target = ALL_HODL[['Response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HODL_Feats = ALL_HODL.drop(['Response'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HODL_Feats = HODL_Feats.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HODL_Feats = HODL_Feats.drop(['Id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "6913/6913 [==============================] - 1s - loss: 0.6209 - acc: 0.6835     \n",
      "Epoch 2/15\n",
      "6913/6913 [==============================] - 1s - loss: 0.5449 - acc: 0.7064     \n",
      "Epoch 3/15\n",
      "6913/6913 [==============================] - 1s - loss: 0.5215 - acc: 0.7278     \n",
      "Epoch 4/15\n",
      "6913/6913 [==============================] - 1s - loss: 0.5109 - acc: 0.7282     \n",
      "Epoch 5/15\n",
      "6913/6913 [==============================] - 0s - loss: 0.5062 - acc: 0.7282     \n",
      "Epoch 6/15\n",
      "6913/6913 [==============================] - 0s - loss: 0.5040 - acc: 0.7282     \n",
      "Epoch 7/15\n",
      "6913/6913 [==============================] - 1s - loss: 0.5026 - acc: 0.7282     \n",
      "Epoch 8/15\n",
      "6913/6913 [==============================] - 1s - loss: 0.5018 - acc: 0.7282     \n",
      "Epoch 9/15\n",
      "6913/6913 [==============================] - 0s - loss: 0.5012 - acc: 0.7282     \n",
      "Epoch 10/15\n",
      "6913/6913 [==============================] - 1s - loss: 0.5008 - acc: 0.7282     \n",
      "Epoch 11/15\n",
      "6913/6913 [==============================] - 0s - loss: 0.5005 - acc: 0.7282     \n",
      "Epoch 12/15\n",
      "6913/6913 [==============================] - 0s - loss: 0.5003 - acc: 0.7282     \n",
      "Epoch 13/15\n",
      "6913/6913 [==============================] - 0s - loss: 0.5003 - acc: 0.7282     \n",
      "Epoch 14/15\n",
      "6913/6913 [==============================] - 0s - loss: 0.5003 - acc: 0.7282     \n",
      "Epoch 15/15\n",
      "6913/6913 [==============================] - 1s - loss: 0.5000 - acc: 0.7282     \n"
     ]
    }
   ],
   "source": [
    "ANN_preds = ANN(X_train=ALL_T_Feats, y_train=ALL_T_Target, X_test=HODL_Feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5122838944494995,\n",
       " 0.48771610555050043,\n",
       " 1.0,\n",
       " 0.5301681503461919,\n",
       " 0.4698318496538081,\n",
       " 0.0,\n",
       " 0.14102564102564102,\n",
       " 0.25740694973463885)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DoConfMat(yTestIn=HODL_Target, yPredIn=ANN_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging bootstrap under sampling to deal with class imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DoGB_default(X_trainIn, y_trainIn, X_testIn):\n",
    "    model = GradientBoostingClassifier()\n",
    "    model.fit(X_trainIn, y_trainIn)\n",
    "    D_y_predictions = model.predict(X_testIn)\n",
    "    return(D_y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bootstrap_underSamp(trainIn, testIn, num_boots, cutOff):\n",
    "    pred_collect = []\n",
    "    testX = testIn.iloc[:,:-1]\n",
    "    #print testX.shape\n",
    "    for i in range(0,num_boots):\n",
    "        \n",
    "        # down sample positive samples by 0.725 to 0.875\n",
    "        # thos np.random numbers determine the amount of under sampling from positive and negative classes. \n",
    "        #num_samp_pos = int(float(round(trainIn[trainIn.Response == 1].shape[0])) * np.random.uniform(0.725,0.875)) \n",
    "        #num_samp_neg = int(float(round(trainIn[trainIn.Response == 0].shape[0])) * np.random.uniform(0.05,0.125))\n",
    "        num_samp_pos = int(float(round(trainIn[trainIn.Response == 1].shape[0])) * np.random.uniform(0.725,0.875)) \n",
    "        num_samp_neg = int(float(round(trainIn[trainIn.Response == 0].shape[0])) * np.random.uniform(0.725,0.875))\n",
    "        downNeg = DownSample_Negatives(trainIn, num_samp_neg)\n",
    "        downPos = DownSample_Positives(trainIn, num_samp_pos)\n",
    "        recomb = pd.concat([downNeg,downPos])\n",
    "        #print recomb.shape\n",
    "\n",
    "        trainX = recomb.iloc[:,:-1]\n",
    "        trainY = recomb.iloc[:,-1]\n",
    "\n",
    "        pred = DoGB_default(X_trainIn=trainX, y_trainIn=trainY, X_testIn=testX)\n",
    "        #pred= DoGB(X_trainIn=trainX, y_trainIn=trainY, X_testIn=testX,a=948, b=34, c=0.165, d=0.878)\n",
    "\n",
    "        pred_collect.append(pred)\n",
    "    pred_fractions = map(np.mean, zip(*pred_collect))\n",
    "    pred = [1 if x >= cutOff else 0 for x in pred_fractions]\n",
    "    return(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_HODL = ALL_HODL.drop(['Id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_TT = ALL_TT.drop(['Id', 'index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BSUS= Bootstrap_underSamp(trainIn=all_TT, testIn=all_HODL, num_boots=10, cutOff=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7388535031847133,\n",
       " 0.2611464968152866,\n",
       " 0.9810606060606061,\n",
       " 0.28222881635344543,\n",
       " 0.7177711836465546,\n",
       " 0.01893939393939394,\n",
       " 0.23228699551569507,\n",
       " 0.40090919262370722)"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DoConfMat(yTestIn=HODL_Target, yPredIn=BSUS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod1 = RandomForestClassifier(criterion=\"gini\")\n",
    "mod2 = AdaBoostClassifier()\n",
    "mod3 = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack = [mod1, mod2, mod3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Stacker(trainIn, testIn, stackIn, cutOff):\n",
    "    pred_collect = []\n",
    "    # preprocess\n",
    "    testX = testIn.iloc[:,:-1]\n",
    "    trainX = trainIn.iloc[:,:-1]\n",
    "    trainY = trainIn.iloc[:,-1]\n",
    "    \n",
    "    for ind in range(0,len(stackIn)):\n",
    "        clf =stackIn[ind]\n",
    "        moddy = clf.fit(trainX, trainY)\n",
    "        pred_y = moddy.predict(testX)\n",
    "        pred_collect.append(pred_y)\n",
    "        \n",
    "    pred_fractions = map(np.mean, zip(*pred_collect))\n",
    "    pred = [1 if x >= cutOff else 0 for x in pred_fractions]\n",
    "    return pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predStack = Stacker(trainIn=all_TT, testIn=all_HODL, stackIn=stack, cutOff=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7699423718531999,\n",
       " 0.2300576281468001,\n",
       " 0.9583333333333334,\n",
       " 0.2464556544675239,\n",
       " 0.7535443455324761,\n",
       " 0.041666666666666664,\n",
       " 0.25287356321839083,\n",
       " 0.42024424326199672)"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DoConfMat(yTestIn=HODL_Target, yPredIn=predStack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boot strap under sample stacker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bootstrap_underSamp_Stacker(trainIn, testIn, num_boots, cutOff):\n",
    "    pred_collect = []\n",
    "    #testX = testIn.iloc[:,:-1]\n",
    "    #print testX.shape\n",
    "    for i in range(0,num_boots):\n",
    "        \n",
    "        # down sample positive samples by 0.725 to 0.875\n",
    "        # thos np.random numbers determine the amount of under sampling from positive and negative classes. \n",
    "        #num_samp_pos = int(float(round(trainIn[trainIn.Response == 1].shape[0])) * np.random.uniform(0.725,0.875)) \n",
    "        #num_samp_neg = int(float(round(trainIn[trainIn.Response == 0].shape[0])) * np.random.uniform(0.05,0.125))\n",
    "        #print trainIn.shape\n",
    "        num_samp_pos = int(float(round(trainIn[trainIn.Response == 1].shape[0])) * np.random.uniform(0.725,0.875)) \n",
    "        num_samp_neg = int(float(round(trainIn[trainIn.Response == 0].shape[0])) * np.random.uniform(0.725,0.875))\n",
    "        downNeg = DownSample_Negatives(trainIn, num_samp_neg)\n",
    "        downPos = DownSample_Positives(trainIn, num_samp_pos)\n",
    "        recomb = pd.concat([downNeg,downPos])\n",
    "        #print recomb.shape\n",
    "        \n",
    "        #recomb = recomb.iloc[:,:-1]\n",
    "        #print type(testX)\n",
    "        #print type(recomb)\n",
    "        #print recomb.shape\n",
    "        pred = Stacker(trainIn=recomb, testIn=testIn, stackIn=stack, cutOff=0.5)\n",
    "\n",
    "        #trainX = recomb.iloc[:,:-1]\n",
    "        #trainY = recomb.iloc[:,-1]\n",
    "\n",
    "        #pred = DoGB_default(X_trainIn=trainX, y_trainIn=trainY, X_testIn=testX)\n",
    "        #pred= DoGB(X_trainIn=trainX, y_trainIn=trainY, X_testIn=testX,a=948, b=34, c=0.165, d=0.878)\n",
    "\n",
    "        pred_collect.append(pred)\n",
    "    pred_fractions = map(np.mean, zip(*pred_collect))\n",
    "    pred = [1 if x >= cutOff else 0 for x in pred_fractions]\n",
    "    return(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BSUS2= Bootstrap_underSamp_Stacker(trainIn=all_TT, testIn=all_HODL, num_boots=10, cutOff=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7622080679405521,\n",
       " 0.23779193205944799,\n",
       " 0.9678030303030303,\n",
       " 0.25568743818001977,\n",
       " 0.7443125618199802,\n",
       " 0.032196969696969696,\n",
       " 0.2478176527643065,\n",
       " 0.41689758384241588)"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DoConfMat(yTestIn=HODL_Target, yPredIn=BSUS2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Had I more time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Applied K-means clustering to find underlying structure in data - and see if data can be split by certain categories \n",
    "### - Optimized my models with other techniques other than Grid Search / Random search\n",
    "### - Investigated effects of perturbing the data with different levels of poor quality data\n",
    "### - Ran the genetic algorithm for enough iterations to achieve stronger performance\n",
    "### - Put more time into accurate imputation via regression + optimized regression models. \n",
    "### - Imputed categorical columns with classification\n",
    "### - Look further into what the data is ... explore time series components if it exist + investigate of autocorrelation\n",
    "### - Look at dealing with class imbalance with cost assymetry + more advanced sampling techniqies (eg. SMOTE)\n",
    "### - More visualisations eg. ROC Curves.\n",
    "### - Explore effect of PCA further\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff I threw away - eg. Particle Swarm Optimization - They following code does not work - I did not finish because I did not want to take advantage of your patience - but I'll keep it as proof of effort..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#   If the fitness value is better than the best fitness value (pBest) in history\n",
    "#            set current value as the new pBest\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "        # pick a random number between 0 and 1\n",
    "        # if \n",
    "        \n",
    "        paramms = RandomSearch()\n",
    "        self.costy =  paramms[0] #float(np.random.uniform(low=-1, high=5, size=1)) \n",
    "        self.gammax = paramms[1] #float(np.random.uniform(low=-4, high=-1, size=1)) \n",
    "        self.x_pos = np.exp(self.gammax)\n",
    "        self.y_pos = np.exp(self.costy)\n",
    "        self.fitness_Value = paramms[2]\n",
    "        self.velocity_X = 0\n",
    "        self.velocity_Y = 0 \n",
    "\n",
    "        # each particle keeps a track of it's own local best.\n",
    "        self.particle_tracker = []\n",
    "        self.pbest = paramms[2]\n",
    "\n",
    "        self.max_X_boundary = 10.0 ** 6 \n",
    "        self.min_X_boundary = 10.0 ** -6 \n",
    "        self.max_Y_boundary = 10.0 ** 6\n",
    "        self.min_Y_boundary = 10.0 ** -6\n",
    "        \n",
    "        \n",
    "    def change_position(self, new_x, new_y):\n",
    "        self.x_pos = new_x\n",
    "        self.y_pos = new_y\n",
    "        #global x_pos\n",
    "        #global y_pos\n",
    "        #x_pos = int(np.random.uniform(low=1, high=100, size=1)) \n",
    "        #y_pos = int(np.random.uniform(low=1, high=100, size=1))\n",
    "    def change_velocity(self, heatX, heatY):\n",
    "\n",
    "\n",
    "        #v[] = v[] + c1 * rand() * (pbest[] - present[]) + c2 * rand() * (gbest[] - present[]) (a)\n",
    "        #present[] = persent[] + v[] (b)\n",
    "\n",
    "        #v[] is the particle velocity \n",
    "        #persent[] is the current particle (solution). \n",
    "        #pbest[] and gbest[] are defined as stated before. \n",
    "        #rand () is a random number between (0,1). \n",
    "        #c1, c2 are learning factors. usually c1 = c2 = 2. \n",
    "        r1 = float(np.random.uniform(low=0.0, high=1.0, size=1))\n",
    "        r2 = float(np.random.uniform(low=0.0, high=1.0, size=1))  \n",
    "        c1 = 2.0\n",
    "        c2 = 2.0\n",
    "        pbest_vs_pres_X = self.pbest[1] - self.x_pos # local best X - particle X\n",
    "        pbest_vs_pres_Y = self.pbest[2] - self.y_pos # local best Y - paticle Y\n",
    "        gbest_vs_pres_X = heatX - self.x_pos # global best X - particle X\n",
    "        gbest_vs_pres_Y = heatY - self.y_pos # gloabl best Y - particle Y\n",
    "\n",
    "        pbest_update_X = c1 * r1 * float(pbest_vs_pres_X)\n",
    "        gbest_update_X = c2 * r2 * float(gbest_vs_pres_X)\n",
    "        pbest_update_Y = c1 * r1 * float(pbest_vs_pres_Y)\n",
    "        gbest_update_Y = c2 * r2 * float(gbest_vs_pres_Y)\n",
    "\n",
    "        X_vel_change = float(self.velocity_X) + float(pbest_update_X) + float(gbest_update_X)\n",
    "        Y_vel_change = float(self.velocity_Y) + float(pbest_update_Y) + float(gbest_update_Y)\n",
    "\n",
    "        potential_new_x_pos = self.x_pos + float(X_vel_change)\n",
    "        potential_new_y_pos = self.y_pos + float(Y_vel_change)\n",
    "\n",
    "        ### sometimes this does not work - got to get that rule right! \n",
    "        #self.x_pos = potential_new_x_pos\n",
    "        #self.y_pos = potential_new_y_pos\n",
    "\n",
    "        if potential_new_x_pos > self.min_X_boundary and potential_new_x_pos <= self.max_X_boundary:\n",
    "            self.x_pos = potential_new_x_pos\n",
    "        else:\n",
    "            if potential_new_x_pos <= self.min_X_boundary:\n",
    "                potential_new_x_pos = self.x_pos + X_vel_change\n",
    "                dif_to_min = abs(potential_new_x_pos)\n",
    "                self.x_pos = dif_to_min\n",
    "            if potential_new_x_pos > self.max_X_boundary:\n",
    "                potential_new_x_pos = self.max_X_boundary - (self.x_pos + X_vel_change) \n",
    "                dif_to_min = abs(potential_new_x_pos) \n",
    "                self.x_pos = dif_to_min\n",
    "\n",
    "\n",
    "        if potential_new_y_pos > self.min_Y_boundary and potential_new_y_pos <= self.max_Y_boundary:\n",
    "            self.y_pos = potential_new_y_pos\n",
    "        else:\n",
    "            if potential_new_y_pos <= self.min_Y_boundary:\n",
    "                potential_new_y_pos = self.y_pos + Y_vel_change\n",
    "                dif_to_min = abs(potential_new_y_pos)\n",
    "                self.y_pos = dif_to_min\n",
    "            if potential_new_y_pos > self.max_Y_boundary:\n",
    "                potential_new_y_pos = self.max_Y_boundary - (self.y_pos + Y_vel_change) \n",
    "                dif_to_min = abs(potential_new_y_pos) \n",
    "                self.y_pos = dif_to_min\n",
    "\n",
    "\n",
    "    def track_results(self, EvalScoreIn, CurrentX, CurrentY):\n",
    "        self.particle_tracker.append((EvalScoreIn, CurrentX, CurrentY))\n",
    "\n",
    "    def update_pbest(self):\n",
    "        current_tracker = self.particle_tracker\n",
    "        current_tracker = list(reversed(sorted(current_tracker)))\n",
    "        current_local_best = current_tracker.pop()  # finds minimum eval score\n",
    "        self.pbest = current_local_best\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "P1 = Particle(\"Particle 1\")\n",
    "P2 = Particle(\"Particle 2\")\n",
    "P3 = Particle(\"Particle 3\")\n",
    "P4 = Particle(\"Particle 4\") \n",
    "P5 = Particle(\"Particle 5\")\n",
    "P6 = Particle(\"Particle 6\")\n",
    "P7 = Particle(\"Particle 7\")\n",
    "P8 = Particle(\"Particle 8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# downsampling\n",
    "from /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages import imblearn\n",
    "sys.path.append('/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages')\n",
    "import sys, os.path\n",
    "imblearn_dir = (os.path.abspath(os.path.join(os.path.dirname(__file__), '..')) + \n",
    "                '/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages')\n",
    "sys.path.append(imblearn_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit model with bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit a model with stacking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
